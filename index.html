<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset AND Benchmark">
  <meta name="keywords" content="SMPLX, Sign Language Datasets, Sign Language Production">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset AND Benchmark</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h4 class="title conference-title is-4">NeurIPS 2023</h4> -->
            <h2 class="title is-2 publication-title">
              SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset AND Benchmark
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://zhengdiyu.github.io/">Zhengdi Yu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://ailingzeng.site/">Shaoli Huang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://shunlinlu.github.io/">Yongkang Cheng</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/caiyuanhao1998">Tolga Birdal</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Imperial College London,</span>
              <span class="author-block"><sup>2</sup>Tencent AI Lab</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.00818.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/IDEA-Research/Motion-X" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://docs.google.com/forms/d/e/1FAIpQLSeb1DwnzGPxXWWjXr8cLFPAYd3ZHlWUtRDAzYoGvAKmS4uBlA/viewform" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data Access</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://docs.google.com/document/d/1xeNQkkxD39Yi6pAtJrFS1UcZ2LyJ6RBwxicwQ2j3-Vs/edit?usp=sharing" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>License</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h5 class="subtitle is-5">
        <b>SignAvatars</b> is a large-scale 3D expressive whole-body motion dataset, which comprises 13.7M precise 3D whole-body pose annotations (i.e., SMPL-X) 
        covering 96K motion sequences from massive scenes, meanwhile providing corresponding semantic labels and pose descriptions.

        </h5>
        <img src="./static/images/teaser.png" autoplay muted loop playsinline height="100%">
        <h2 class="subtitle has-text-centered">
          Figure 1: Different from (a) previous motion dataset, 
          (b) our dataset captures body, facial expressions, and hand gestures. 
          We highlight the comparisons of facial expressions and hand gestures.
        </h2>
        <img src="./static/images/comp_data.png" autoplay muted loop playsinline height="100%">
      </div>
    </div>
  </section>

  <head>
    <style>
        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 70vh; /* 70% of the viewport height */
        }
        .video-container iframe {
            width: 800px; /* 70% of the container's width */
            height: 450px; /* Maintain the aspect ratio */
        }
    </style>
</head>
<body>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/0a0ZYJgzdWE" frameborder="0" allowfullscreen></iframe>
    </div>
</body>

<p style="margin-bottom: -0.5cm;"></p>


<p style="margin-bottom: -0.5cm;"></p>

<body>
  <div class="video-container">
      <iframe src="https://www.youtube.com/embed/QWoll6asFhE" frameborder="0" allowfullscreen></iframe>
  </div>
</body>
  
<p style="margin-bottom: -1.5cm;"></p>
  
  <!-- <p style="margin-bottom: -1.5cm;"></p> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We propose <b>Motion-X</b>, a large-scale 3D expressive whole-body motion dataset. 

      
            </p>
            <p>
              Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. 
              Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, limiting their scalability.
              To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos 
              and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. 
              This pipeline is of high precision, cost-effective, and scalable for further research. 
            </p>
            <p>
              Based on it, we construct <b>Motion-X</b>, which comprises <b>13.7M</b> precise 3D whole-body <b>pose annotations</b> (i.e., SMPL-X) covering <b>96K</b> motion sequences from massive scenes. 
              Besides, Motion-X provides <b>13.7M frame-level</b> whole-body pose descriptions and <b>96K sequence-level</b> semantic labels.
            </p>
            <p>
              Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as the 3D whole-body human mesh recovery task.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Motion-X Dataset</h3>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/overview.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Figure 2: SMPL-X motion samples from the Motion-X dataset. 
        </h2>
        <img src="./static/images/vis_annotation.png" autoplay muted loop playsinline height="100%">
        <h2 class="subtitle has-text-centered">
        Figure 3: Overview of Motion-X: (a) diverse facial expressions, (b) indoor motion with expressive face and hand motions, 
        (c) outdoor motion with challenging poses, and (d) several motion sequences. 
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Visualization of motion annotations from massive online videos</h3>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
         <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/kungfu____images____subset_0____Aerial_Kick_Kungfu_wushu_clip15____Aerial_Kick_Kungfu_wushu_clip15.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.patreon.com/mastersongkungfu">website</a>.
        </h2>
         <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/kungfu____images____subset_0____Aerial_Kick_Kungfu_wushu_clip12____Aerial_Kick_Kungfu_wushu_clip12.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.patreon.com/mastersongkungfu">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/perform____images____cross_the_single_plank_bridge____cross_the_single_plank_bridge_subset_1.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.xiaohongshu.com/user/profile/5ec2aac700000000010059c0/618e6c7f000000000102e60b">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/perform____images____cross_the_single_plank_bridge____cross_the_single_plank_bridge_subset_3.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.xiaohongshu.com/user/profile/5ec2aac700000000010059c0/618e6c7f000000000102e60b">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/perform____images____dance_clip10____dance_clip10.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.xiaohongshu.com/user/profile/5ec2aac700000000010059c0/618e6c7f000000000102e60b">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/game_motion____images____subset_0____A_narrow_roadNlotion_ActorlncNao_clip2.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://space.bilibili.com/2026611047?spm_id_from=333.337.0.0">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/game_motion____images____subset_0____A_Monster_Powers_Up_Motion_Actorlnc_Sugiguchi_Hideki_clip1.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://space.bilibili.com/2026611047?spm_id_from=333.337.0.0">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/animation_actions____images____subset_0____Horse_clip1.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.youtube.com/channel/UCzgkpehSWuFTQx9E8NkBqzw">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/Being_Chased_clip1.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.youtube.com/channel/UCzgkpehSWuFTQx9E8NkBqzw">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/Better_View_clip1.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.youtube.com/channel/UCzgkpehSWuFTQx9E8NkBqzw">website</a>.
        </h2>
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/perform_ballet_clip39.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.youtube.com/c/elenifit">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/perform_ballet_clip20.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://www.youtube.com/c/elenifit">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v2/music____images____subset_0____Ancient_Drum_clip45____Ancient_Drum_clip45.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="">website</a>.
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/Play_the_stringed_guqin_clip7.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          RGB video is from this <a href="https://search.bilibili.com/all?keyword=%E5%8F%A4%E7%90%B4%E7%A0%94%E4%B9%A0%E7%A4%BE&from_source=webtop_search&spm_id_from=333.1007&search_source=5">website</a>.
        </h2>
        <br>
          <img src="./static/images/whole_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 4: Illustration of the overall data collection and annotation pipeline.
          </h2
        <br/>
        <br>
          <img src="./static/images/motion_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 5: Illustration of the motion annotation pipleine. 
          </h2>
        <br/>
        <br>
          <img src="./static/images/text_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 6: Illustration of (a) whole-body pose description annotation, (b) an example of the text labels.
          </h2>
        <br/>
      
      </div>
    </div>
  </section>
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">UBody Dataset</h3>
        <h5 class="subtitle is-5">
          UBody is a large-scale Upper-Body dataset with the following annotations: 
          <style>
            ul {
              list-style-type: circle;
            }
          </style>
          <ul>
            <li>2D whole-body keypoints</li>
            <li>3D SMPLX annotations</li>
            <li>frame validity label </li>
            <li>person bbox, hand bbox </li>
          </ul>
        </h5>
        <p align="middle">
          <img src="./static/videos_v1/demo_video.gif" width="720" height="240">
        </p>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/demo_video.gif" type="video/mp4">
        </video> -->
      </div>
    </div>
  </section> -->

  <p style="margin-bottom: -0.5cm;"></p>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Examples of the sub-datasets in Motion-X</h3>
        <br>
          <img src="./static/images/stat_motionx.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 7: Motion-X is a superset consisting of eight public datasets and 15K online videos. 
            We annotate all data via our proposed motion and text annotation methods.
          </h2>
        <br/>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <!-- <p align="middle">
          <img src="./static/videos_v1/online_video.mp4" width="720" height="240">
        </p> -->
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/online_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Online Videos
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/aist.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Multi-View Datasest (AIST++ and NTU-RGBD120)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%" width="870">
          <source src="./static/videos_v1/grab.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Human-scene-interaction Datasets (GRAB and EgoBody)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/HAA500.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Action Recognition Datasets (HAA500 and HuMMan)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos_v1/humanml.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Body-Only Motion Capture Datasets (AMASS)
        </h2>

      </div>
    </div>
  </section>


  <!-- <h2 class="title">Contact Us</h2>
<style>
  ul {
    list-style-type: circle;
  }
</style>
<ul>
  <li>For detailed questions about this work, please contact Jing Lin (jinglin.stu@gmail.com).</li>
  <li>We are looking for talented, motivated, and creative research and engineering interns working on human-centric visual understanding and generation topics. If you are interested, please send your CV to Ailing Zeng (zengailing@idea.edu.cn).</li>
</ul>
    </div>
  </section> -->



  <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
    <source src="./static/videos_v1/demo_video.gif" type="video/mp4">
  </video> -->



  <p style="margin-bottom: -1.5cm;"></p>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-size-6">
  </code></pre>
  <h2 class="title">License</h2>
  <style>
    ul {
      list-style-type: circle;
    }
  </style>
  <ul>
    All data is distributed under the CC BY-NC-SA (Attribution-NonCommercial-ShareAlike) license. For the sub-datasets, although we annotate the motion-text labels with our annoation pipeline, we would ask the user to read the original license of each original dataset, 
      and we would only provide our annotated result to the user with the approvals from the original Institution. 
      Here we provide the link of the used assets:  </li>
    <li> <a href="https://mimoza.marmara.edu.tr/~cigdem.erdem/BAUM1/">BAUM</a>, <a href="https://google.github.io/aistplusplus_dataset/">AIST++</a>, 
      <a href="https://sanweiliti.github.io/egobody/egobody.html">EgoBody</a>  datasets are CC-BY 4.0 licensed. </li>
    <li> <a href="https://www.cse.ust.hk/haa/">HAA500</a> dataset is MIT licensed.  </li>
    <li> <a href="https://caizhongang.github.io/projects/HuMMan/">HuMMan</a> dataset is under S-Lab License v1.0. </li>
    <li> <a href="https://rose1.ntu.edu.sg/dataset/actionRecognition/">NTU-RGBD120</a>, 
      <a href="https://grab.is.tue.mpg.de/">GRAB</a>, <a href="https://amass.is.tue.mpg.de/">AMASS</a> dataset is released for academic research only and is free to researchers from educational or research institutes for non-commercial purposes. </li>
    <li> Other data is under CC BY-SA 4.0 license. </li>

  </ul>
      </div>
    </section>
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-size-6">
            <div class="content">
              <p>
                This website is created with this <a href="https://nerfies.github.io/">template</a>. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

  </body>

  </html>
